{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Exploring Spreadsheets and Tables with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Holloway group, we often work with data in spreadsheets and tables. These may be Excel worksheets, CSV files, or some other file type. In this tutorial, we will learn all the basics to work with spreadsheets and tables in Python using the pandas package. By the end of this tutorial, you will be able to:\n",
    "* open and read existing .csv and .xlsx files\n",
    "* create your own pandas DataFrame and save it as both a .csv and .xlsx file\n",
    "* sort and index pandas DataFrames\n",
    "* perform basic mathmatical operations on DataFrame data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas package is a Python package that specifically analyzes and manipulates data in 2D or 1D arrays. Basically, pandas is the go-to tool for looking at data in a table (2D) or list (1D) format. Of course, there is some special vocab to learn with pandas (or any Python package). \n",
    "\n",
    "In an Excel file, a 2D array is called a spreadsheet. In pandas, these 2D arrays are called *DataFrames*. pandas also has a name for a 1D array: a *Series*. Certain commands only work on Series and not DataFrames, or vice versa, so it is important to know which kind of object you are working with.\n",
    "\n",
    "**Note:** the package name pandas is always lowercase. This is a purposeful decision by the creators of pandas. Other Python packages have funny upper- and lowercasing, too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, as with any Python package, you must import your package before using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # we'll be looking at our data, so we'll import pyplot here as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening files with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether your data is in html, csv, json, or xlsx, pandas can open that! Here is a comprehensive list of all file types pandas can open and read: https://pandas.pydata.org/docs/reference/io.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to try opening a few different files. Each file name is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of files to open\n",
    "excel_file = 'Weather.xlsx'\n",
    "csv_file = 'temp.csv'\n",
    "txt_file = 'moisture.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we open these files with pandas, pause for a moment and open one of them using Excel or TextEditor and just see what it looks like. This way, you can check if the pandas version is consistent with the native file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the read_ command to open the three files. Note: use a different command for excel versus csv/txt!\n",
    "excel_data = pd.read_excel(excel_file)\n",
    "csv_data = pd.read_csv(csv_file, sep=',') \n",
    "txt_data = pd.read_csv(txt_file,sep=',')\n",
    "# the sep=',' argument refers to the separator between data, meaning that values in the file are separated by \",\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out each DataFrame by replacing the name below. You don't need to use the print command here.\n",
    "excel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! We opened each file! However, if you looked at the text file, you might have noticed some funny stuff in the first few rows. What happened there? If you open the file in a text editor, you'll see there are three lines of text before the data starts. We don't want that! In order to open the text file without those three rows, we can tell pandas where to start reading in the data. It will skip those three rows of text for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the header argument to name the row to start on.\n",
    "# remember, Python starts counting at 0. So the 4th row is actually 3!\n",
    "txt_data = pd.read_csv(txt_file,sep=',',header=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to make sure the DataFrame looks nice.\n",
    "txt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful coding practice is to limit the number of rows pandas shows you. If you have a large dataset, but you just want to give it a quick check, you can just load first or last couple rows using \".head()\" or \".tail()\". Try it out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have three DataFrames each with different types of weather data for the year 2019. But do we actually care about all these variables? And what if we want to compare variables between DataFrames?\n",
    "\n",
    "In order to simplify things, we can just select the variables we care about. The next few code cells will walk you through the process of selecting columns from each existing DataFrame. After you select the data, you can put these together to make a new DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting columns and making a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's recap the variables we have by printing out the column labels for each DataFrame.\n",
    "print('Excel Variables:')\n",
    "print(excel_data.columns)\n",
    "print('CSV Variables:')\n",
    "print(csv_data.columns)\n",
    "print('TXT Variables:')\n",
    "print(txt_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the variables you care about, then use those column headers to select the data.\n",
    "# you can select different variables here if you want, \n",
    "# but you'll need to make sure you follow these changes throughout the code.\n",
    "var1 = excel_data['Dates']\n",
    "var2 = excel_data['Sky Condition (oktas)']\n",
    "var3 = csv_data['airtemp_degc']\n",
    "var4 = txt_data['liqprec_mm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always check your data to make sure things worked properly.\n",
    "var1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have four pandas Series. You might notice that Series look different than DataFrames. First of all, there is no column header! This is because a Series is 1D data - it is a list. You don't need a column header if you only have a list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than work with four different Series, lets make a new DataFrame! Remember dicts from Tutorial 1? First, we'll make a dict with our four Series, then we'll use that dict to make a DataFrame.\n",
    "\n",
    "Important note: we are able to do this because the Series we extracted have same length! In practice, if you have different length, the missing spots will be filled with a filler value, called NaN (more on this later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dict.\n",
    "data = {'Dates':var1,'Sky Cond (oktas)':var2,'Temp (˚C)':var3,'Precip (mm)':var4}\n",
    "\n",
    "# use that dict to make a DataFrame.\n",
    "df1 = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the df to make sure it worked.\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the data you want, this is a good time to save your data! You can save as any kind of file that pandas can open, but let's try .xlsx and .csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Note: If you get an error message here from the .to_excel command below, you may need to install openpyxl. This is another Python package and it specifically reads and writes Excel data. To install, follow these instructions: <font/>\n",
    "<font color=red>\n",
    "1. copy and paste these instructions to a text document\n",
    "2. close and halt this tutorial and quit jupyter\n",
    "3. navigate to terminal (Mac) or anaconda prompt (Windows)\n",
    "4. use the \"conda install openpyxl\" command \n",
    "5. restart jupyter and this tutorial\n",
    "6. uncomment out the openpyxl import line below.\n",
    "<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we perform the to_csv and to_excel command on df1, then provide a file name and path for saving the data.\n",
    "# specify index=False to remove the DataFrame indexes from the saved file. Just makes things look nicer.\n",
    "# from openpyxl import Workbook\n",
    "df1.to_csv('MyData.csv',index=False)\n",
    "df1.to_excel('MyData.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have data. Now what do we do with it? First of all, we should probably remove all those NaNs. In Python, NaN means Not a Number. It is a placeholder for missing data. If there are a lot of NaNs in your DataFrame, it can be a pain to remove them one by one. Fortunately, pandas has an easy tool to remove NaNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a reminder, this is what the DataFrame looks like.\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dropna command will remove rows OR columns with NaNs. We must specify how we want to data handled. \n",
    "# we'll try it both ways below and see which makes sense!\n",
    "df2 = df1.dropna(axis='index')\n",
    "df3 = df1.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the data. Which method removed rows and which method removed columns with NaNs? \n",
    "# which method do you think makes more sense in this case?\n",
    "print(df2)\n",
    "print('-------------------------------------------------------------')\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we lost all the data in df3! This is because each column with meteorological data had NaNs in it, so the dropna command removed those columns. Instead, we want to use df2 and the axis='index' option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick step back. We made 3 DataFrames in this tutorial. Two of these DataFrames were made from the first DataFrame. Was this necessary? Why did we have to make new DataFrames? \n",
    "\n",
    "pandas is really cool because it can let you edit a DataFrame in place OR make a copy. When you edit a DataFrame in place, this means you change something about the DataFrame without having to change the name or make a new DataFrame. Making a copy is when you basically duplicate the DataFrame, then make the edit to the new DataFrame. \n",
    "\n",
    "To change whether you edit in place or make a copy, all you have to do is specify that inplace=True in the key-word arguments. If we had set inplace=True in our dropna example, we would not have been able to test out both axis options. In this situation, it was really helpful to make a copy! But if we know for sure what we want to do, inplace=True can be helpful for saving memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting, indexing, and subselecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good data, let's explore it a little. Maybe I want to look at only hours with temperatures below 0 ˚C. I need to use the \".loc[condition]\" command to subselect the data.\n",
    "\n",
    "With the loc command, the order of information goes [index, columns]. If you want to select all of the DataFrame rows (meaning all of the indexes), you can skip the index condition and jump right to the column label and condition. This is what we will do in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select only cold data with .loc[] command.\n",
    "# the condition in the brackets says 'anywhere in df2 where the Temp column is less than 0.0 ˚C'.\n",
    "cold = df2.loc[df2['Temp (˚C)']<=0.0]\n",
    "cold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the Sky Conditions on these cold days. \n",
    "# a histogram .hist() is a great tool for quickly assessing the distribution of a variable.\n",
    "# you could also look at precip by changing the column name.\n",
    "plt.hist(cold['Sky Cond (oktas)'].values)\n",
    "plt.xlabel('Sky Cover (oktas)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sky Cover on Cold Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about only warm days?\n",
    "warm = df2.loc[df2['Temp (˚C)']>0.0]\n",
    "warm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are the sky conditions different?\n",
    "plt.hist(warm['Sky Cond (oktas)'].values)\n",
    "plt.xlabel('Sky Cover (oktas)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sky Cover on Warm Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has some really helpful tricks when working with datetime objects, so we can select data based on the Dates column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first, set the Dates column as an index for the DataFrame.\n",
    "df2_dates = df2.set_index(['Dates'])\n",
    "df2_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What changed about the data? Compare df2_dates to df2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when I said the order of the loc command went [index,columns]? Now we will focus on the index conditions. Since we want all of the columns, we can skip this condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's select summer months only.\n",
    "# reminder: summer months are June, July, August (06,07,08).\n",
    "summer = df2_dates.loc['2019-06-01':'2019-08-31'] \n",
    "summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and select fall dates to compare.\n",
    "# fall is definied as September, October, November (09,10,11).\n",
    "fall = df2_dates.loc['2019-09-01':'2019-11-30']\n",
    "fall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average temperature in the summer compared to the fall? What about the max and min precipitation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the average.\n",
    "summer_ave = summer['Temp (˚C)'].mean()\n",
    "fall_ave = fall['Temp (˚C)'].mean()\n",
    "\n",
    "# find the min value.\n",
    "summer_min = summer['Precip (mm)'].min()\n",
    "fall_min = fall['Precip (mm)'].min()\n",
    "\n",
    "# find the max value.\n",
    "summer_max = summer['Precip (mm)'].max()\n",
    "fall_max = fall['Precip (mm)'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the results. \n",
    "# we can put ints or floats into strings using a trick with %.\n",
    "# the .2f means print two floats after the decimal.\n",
    "# you can also use .d or .E. Try both of these out by editing the print statements. What happened?\n",
    "print('Summer average temp: %.2f' % summer_ave) \n",
    "print('Fall average temp: %.2f' % fall_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summer rainfall range: %.2f to %.2f' % (summer_min,summer_max))\n",
    "print('Fall rainfall range: %.2f to %.2f' % (fall_min,fall_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey! We have negative numbers for our rainfall amount! That isn't possible. Well, we clearly missed something in our quality control. That's ok. We know how to fix that now using the loc command. Simply select the data that has a liquid precipitation amount above 0.0. \n",
    "\n",
    "**Knowledge Check** Go back to the top of this section (Sorting, indexing, and subselecting data). Insert a new Code cell and remove negative liquid precip from df2. Then rerun all the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complex sorting and indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to find the average temperature of each month? pandas has a great command for that also!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use groupby to group the data according to month. Then we tell pandas to take the mean of each group.\n",
    "month_aves = df2_dates.groupby(by=df2_dates.index.month).mean()\n",
    "month_aves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the averages of everything for each month. What if we just want the temperature column? We can do this in two ways. 1) run groupby on the whole DataFrame then select the Temp column, or 2) run groupby on only the Temp column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_aves = df2_dates.groupby(by=df2_dates.index.month).mean()\n",
    "temp1 = month_aves['Temp (˚C)']\n",
    "temp2 = df2_dates['Temp (˚C)'].groupby(by=df2_dates.index.month).mean()\n",
    "print(temp1)\n",
    "print('---------------------')\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods produce the same result, so you can use either in the future!\n",
    "\n",
    "Now let's group by other features. What if we want to group based on hour? Let's try this by finding the maximum value at each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_max = df2_dates.groupby(by=df2_dates.index.hour).max()\n",
    "\n",
    "plt.plot(hour_max['Temp (˚C)'])\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Temperature (˚C)')\n",
    "plt.title('Hourly Average Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird, why is the temperature coldest at 11 am? Because the time is in UTC! We could fix that by making the DatetimeIndex time-zone aware and then converting the time zone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_localized = df2_dates.tz_localize(tz='UTC') # make the DataFrame time-zone aware\n",
    "df3 = df2_localized.tz_convert(tz='US/Central') # convert from UTC to Central time\n",
    "# side note: tz_localize and tz_convert do not have inplace=True options.\n",
    "# we have to make new dfs, but we can just keep calling those copies df2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how the Dates info changed here: \n",
    "print(\"UTC  :\",df2_localized.index[0:4].hour)\n",
    "print(\"Central time:\",df3.index[0:4].hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the hour groupby again\n",
    "hour_max = df3.groupby(by=df3.index.hour).max()\n",
    "plt.plot(hour_max['Temp (˚C)'])\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Temperature (˚C)')\n",
    "plt.title('Hourly Average Temperature in Central Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully by now you feel comfortable playing around with pandas. This is important for a research group like ours because we do a lot of work with 1D and 2D data. To check your knowledge, try rerunning this tutorial but select different variables and use different mathematical commands. First, though, click Kernel -> Restart and Clear Output to give you a clean slate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
